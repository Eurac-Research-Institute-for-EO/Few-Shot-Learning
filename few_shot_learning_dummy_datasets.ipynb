{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-Shot Learning with Presto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Overview \n",
    "\n",
    "1) Short introduction on Foundation Models and Presto\n",
    "2) Definition of Few-Shot learning\n",
    "3) Apply Presto to perfrom Few-Shot learning on a regression and a classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Foundation Models\n",
    "\n",
    "A Foundation Model is a model trained on large and diverse unlabeled datasets to learn general patterns and features of the data. Thanks to its strong generalization capabilities, such a model can be adapted for a wide range of applications that use similar types of input data.\n",
    "\n",
    "**Presto** (**P**retrained **Re**mote **S**ensing **T**ransf**o**rmer) is a foundation model trained on a large, unlabeled dataset of Sentinel-2, Sentinel-1, Meteorological and Topography pixel-timeseries data. It is able to capture long-range relationships across time and sensor dimensions, improving the signal-to-noise ratio and providing a concise, informative representation of the inputs. \n",
    "In this project, We made use of the [Presto](https://github.com/WorldCereal/prometheo.git) version developed in collaboration with WorldCereal\n",
    "\n",
    "Originally trained on monthly composites, Presto has been refined to be able to ingest dekadal data and to be fine-tuned for regression and classification tasks.\n",
    "\n",
    "### 2) Few-Shot Learning\n",
    "\n",
    "Few-shot learning aims to develop models that can learn from a small number of labeled instances while enhancing generalization and performance on new, unseen examples.\n",
    "\n",
    "Given a dataset with only a few annotated examples, we can fine-tune a pretrained foundation model to either directly handle the downstream task or generate compressed representations of the inputs (embeddings), which can then be used to train a machine learning model for the downstream task. We hereby show-case the former scenario, whose overview is depicted in the figure below.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"../images/ScaleAG_pipeline_overview_presto.jpg\" alt=\"Overview of a Foundation Model fine tuned for different downstream tasks and applications.\" width=\"700\" />\n",
    "    <p><em>Overview of a Foundation Model fine tuned for different downstream tasks and applications.</em></p>\n",
    "</div>\n",
    "\n",
    "### 3) Implementing Few-Shot learning with Presto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "from scaleagdata_vito.presto.datasets_prometheo import ScaleAgDataset\n",
    "from scaleagdata_vito.openeo.extract_sample_scaleag import generate_input_for_extractions, extract\n",
    "from scaleagdata_vito.presto.utils import evaluate_finetuned_model\n",
    "from scaleagdata_vito.presto.presto_df import load_dataset\n",
    "from scaleagdata_vito.presto.utils import train_test_val_split, finetune_on_task, load_finetuned_model, get_pretrained_model_url, get_resources_dir\n",
    "from scaleagdata_vito.presto.inference import PrestoPredictor, reshape_result, plot_results\n",
    "from scaleagdata_vito.utils.map import ui_map\n",
    "from scaleagdata_vito.utils.dateslider import date_slider\n",
    "from scaleagdata_vito.openeo.extract_sample_scaleag import collect_inputs_for_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we start...\n",
    "\n",
    "**Check your data!** Investigate validity of geometries uniqueness of sample IDs, presence of outliers and so on before starting the extraction. Achieving good performance making use of a limited amount of data is a challening task per se. Therefore, **the quality of your data will greatly impact your final results.**\n",
    "\n",
    "Data requirements:\n",
    "- Points or Polygons (will be aggregated in points)\n",
    "- Lat-Lon (crs:4326) \n",
    "- Format: parquet, GeoJSON, shapefile, GPKG\n",
    "For each geometry:\n",
    "- Date (if available) \n",
    "- Unique ID\n",
    "- Annotations\n",
    "\n",
    "Good practice:\n",
    "\n",
    "Remove polygons close to borders (e.g. apply buffer) to ensure data are contained in the field\n",
    "If the annotations are accurate, point geometries should be preferred. However, especially in regression tasks (i.e., continuous output values) such us yield estimation the target values might be noisy. In that case, we recommend subdividing the polygons in subfields of 20m x 20m (to cover more measurements) and computing the median yield for a smoother and more reliable target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements for running the extractions\n",
    "- Account in [Copernicus Data Space Ecosystem (CDSE)](https://dataspace.copernicus.eu/). You can sign up for free and have a monthly availability of 10000 credits.\n",
    "- A dataset with valid geometries (Points or Polygons) in lat-lon projection.\n",
    "- Preferably a dataset with unique IDs per sample \n",
    "- A labelled dataset. Not required for the extraction process, but for the following fine-tuning steps.\n",
    "\n",
    "#### EO data extractions\n",
    "In this first step, we extract for each sample in your dataset the required EO time series from CDSE using OpenEO.\n",
    "For running the job, the user should indicate the following job_dictionary fields:\n",
    "\n",
    "```python\n",
    "    job_params = dict(\n",
    "        output_folder=..., # where to save the extracted dataset\n",
    "        input_df=..., # input georeferenced dataset to run the extractions for \n",
    "        start_date=..., # string indicating from which date to extract data  \n",
    "        end_date=..., # string indicating until which date to extract the data \n",
    "        unique_id_column=..., # name of the column in the input_df containing the unique ID of the samples  \n",
    "        composite_window=..., # \"month\" or \"dekad\" are supported. Default is \"dekad\"\n",
    "    )\n",
    "```\n",
    "in particular:\n",
    "- If the `date` information associated with the label is provided, the `start_date` of the time-series is automatically set to 9 months before the date, whereas the `end_date` is set to 9 months after. If `date` is not available, the user needs to manually indicate the desired `start_date` and `end_date` for the extractions.\n",
    "- `composite_window` indicates the time-series granularity, which can be dekadal or monthly. \n",
    "  - `dekad`: each time step in the extracted time series corresponds to a mean-compositing operation on 10-days acquisitions. Accordingly with the start and end date, each month will be covered by 3 time steps which, by default, correspond to the 1st, 11th and 21th of the month. \n",
    "  - `month`: each time step in the extracted time series corresponds to a mean-compositing operation on 30-days acquisitions. Each month will be covered by 1 time step which, by default, correspond to the 1st of the month.\n",
    "\n",
    "The following decadal/monthly time series will be extracted for the indicated time range:\n",
    "\n",
    "- Sentinel-2 L2A data (all bands)\n",
    "- Sentinel-1 VH and VV\n",
    "- Average air temperature and precipitation sum derived from AgERA5\n",
    "- Slope and elevation from Copernicus DEM\n",
    "\n",
    "Presto accepts 1D time-series. Therefore, if Polygons are provided for the extractions, the latter are spatially aggregated in points which will correspond to the centroid lat lon geolocation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression task: potato yield estimation \n",
    "\n",
    "The data covers fields in Belgium during the growing season of 2022. Each field polygon was partitioned in subfields of 20m x 20m. The latter are partitioned into training, validation and test sets. \n",
    "\n",
    "**NOTE:** This is a very small dummy dataset with randomized yield values. No meaningful results are expected from using such data for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = Path(\"/mnt/CEPH_PROJECTS/ScaleAgData/06_abhishek/few_shot/output_AGB/\")\n",
    "input_df = get_resources_dir() / \"Fieldwork_2023_samples_uid.geojson\"\n",
    "start_date = \"2023-04-01\"\n",
    "end_date = \"2023-10-31\"\n",
    "unique_id_column = \"uid\"\n",
    "composite_window = \"dekad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = Path(\"/mnt/CEPH_PROJECTS/ScaleAgData/06_abhishek/few_shot/output_v1/\")\n",
    "input_df1 = get_resources_dir() / \"dummy_yield.geojson\"\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "unique_id_column = \"fieldname\"\n",
    "composite_window = \"dekad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FOI/site</th>\n",
       "      <th>Farm</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>AGB_t/ha</th>\n",
       "      <th>date</th>\n",
       "      <th>parentfield</th>\n",
       "      <th>uid</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R1-1</td>\n",
       "      <td>RNLI23</td>\n",
       "      <td>46.530865</td>\n",
       "      <td>11.431759</td>\n",
       "      <td>0.45524</td>\n",
       "      <td>2023-04-27</td>\n",
       "      <td>R1</td>\n",
       "      <td>1</td>\n",
       "      <td>POINT (11.43176 46.53086)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R1-2</td>\n",
       "      <td>RNLI23</td>\n",
       "      <td>46.530798</td>\n",
       "      <td>11.431515</td>\n",
       "      <td>1.05644</td>\n",
       "      <td>2023-04-27</td>\n",
       "      <td>R1</td>\n",
       "      <td>2</td>\n",
       "      <td>POINT (11.43152 46.5308)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R1-3</td>\n",
       "      <td>RNLI23</td>\n",
       "      <td>46.531055</td>\n",
       "      <td>11.431667</td>\n",
       "      <td>0.70364</td>\n",
       "      <td>2023-04-27</td>\n",
       "      <td>R1</td>\n",
       "      <td>3</td>\n",
       "      <td>POINT (11.43167 46.53106)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R5-1</td>\n",
       "      <td>RNLI23</td>\n",
       "      <td>46.530982</td>\n",
       "      <td>11.434240</td>\n",
       "      <td>0.56164</td>\n",
       "      <td>2023-04-27</td>\n",
       "      <td>R5</td>\n",
       "      <td>4</td>\n",
       "      <td>POINT (11.43424 46.53098)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R5-2</td>\n",
       "      <td>RNLI23</td>\n",
       "      <td>46.531125</td>\n",
       "      <td>11.434305</td>\n",
       "      <td>0.42204</td>\n",
       "      <td>2023-04-27</td>\n",
       "      <td>R5</td>\n",
       "      <td>5</td>\n",
       "      <td>POINT (11.4343 46.53112)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>L1-2</td>\n",
       "      <td>LRKA23</td>\n",
       "      <td>46.453585</td>\n",
       "      <td>11.079331</td>\n",
       "      <td>0.30604</td>\n",
       "      <td>2023-10-19</td>\n",
       "      <td>L1</td>\n",
       "      <td>280</td>\n",
       "      <td>POINT (11.07933 46.45358)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>L1-3</td>\n",
       "      <td>LRKA23</td>\n",
       "      <td>46.453720</td>\n",
       "      <td>11.079279</td>\n",
       "      <td>0.34044</td>\n",
       "      <td>2023-10-19</td>\n",
       "      <td>L1</td>\n",
       "      <td>281</td>\n",
       "      <td>POINT (11.07928 46.45372)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>F1-1</td>\n",
       "      <td>FDCS23</td>\n",
       "      <td>46.433962</td>\n",
       "      <td>11.138077</td>\n",
       "      <td>0.21044</td>\n",
       "      <td>2023-10-19</td>\n",
       "      <td>F1</td>\n",
       "      <td>282</td>\n",
       "      <td>POINT (11.13808 46.43396)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>F1-2</td>\n",
       "      <td>FDCS23</td>\n",
       "      <td>46.433860</td>\n",
       "      <td>11.138437</td>\n",
       "      <td>0.36684</td>\n",
       "      <td>2023-10-19</td>\n",
       "      <td>F1</td>\n",
       "      <td>283</td>\n",
       "      <td>POINT (11.13844 46.43386)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>F1-3</td>\n",
       "      <td>FDCS23</td>\n",
       "      <td>46.433673</td>\n",
       "      <td>11.138334</td>\n",
       "      <td>0.34284</td>\n",
       "      <td>2023-10-19</td>\n",
       "      <td>F1</td>\n",
       "      <td>284</td>\n",
       "      <td>POINT (11.13833 46.43367)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    FOI/site    Farm        lat        lon  AGB_t/ha       date parentfield  \\\n",
       "0       R1-1  RNLI23  46.530865  11.431759   0.45524 2023-04-27          R1   \n",
       "1       R1-2  RNLI23  46.530798  11.431515   1.05644 2023-04-27          R1   \n",
       "2       R1-3  RNLI23  46.531055  11.431667   0.70364 2023-04-27          R1   \n",
       "3       R5-1  RNLI23  46.530982  11.434240   0.56164 2023-04-27          R5   \n",
       "4       R5-2  RNLI23  46.531125  11.434305   0.42204 2023-04-27          R5   \n",
       "..       ...     ...        ...        ...       ...        ...         ...   \n",
       "279     L1-2  LRKA23  46.453585  11.079331   0.30604 2023-10-19          L1   \n",
       "280     L1-3  LRKA23  46.453720  11.079279   0.34044 2023-10-19          L1   \n",
       "281     F1-1  FDCS23  46.433962  11.138077   0.21044 2023-10-19          F1   \n",
       "282     F1-2  FDCS23  46.433860  11.138437   0.36684 2023-10-19          F1   \n",
       "283     F1-3  FDCS23  46.433673  11.138334   0.34284 2023-10-19          F1   \n",
       "\n",
       "     uid                   geometry  \n",
       "0      1  POINT (11.43176 46.53086)  \n",
       "1      2   POINT (11.43152 46.5308)  \n",
       "2      3  POINT (11.43167 46.53106)  \n",
       "3      4  POINT (11.43424 46.53098)  \n",
       "4      5   POINT (11.4343 46.53112)  \n",
       "..   ...                        ...  \n",
       "279  280  POINT (11.07933 46.45358)  \n",
       "280  281  POINT (11.07928 46.45372)  \n",
       "281  282  POINT (11.13808 46.43396)  \n",
       "282  283  POINT (11.13844 46.43386)  \n",
       "283  284  POINT (11.13833 46.43367)  \n",
       "\n",
       "[284 rows x 9 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check input data structure \n",
    "gpd.read_file(input_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parentname</th>\n",
       "      <th>fieldname</th>\n",
       "      <th>median_yield</th>\n",
       "      <th>date</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1574374</td>\n",
       "      <td>1574374-3</td>\n",
       "      <td>8259.734336</td>\n",
       "      <td>2022-09-21 18:54:15</td>\n",
       "      <td>POLYGON ((3.18533 51.01671, 3.18533 51.01689, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1422963</td>\n",
       "      <td>1422963-10</td>\n",
       "      <td>1030.794709</td>\n",
       "      <td>2022-10-25 10:53:03</td>\n",
       "      <td>POLYGON ((5.11628 51.31606, 5.11629 51.31624, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1676341</td>\n",
       "      <td>1676341-3</td>\n",
       "      <td>2266.873831</td>\n",
       "      <td>2022-10-12 14:44:10</td>\n",
       "      <td>POLYGON ((5.03693 51.28708, 5.03693 51.28725, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1650385</td>\n",
       "      <td>1650385-7</td>\n",
       "      <td>1438.154486</td>\n",
       "      <td>2022-09-30 14:21:27</td>\n",
       "      <td>POLYGON ((2.85914 50.92523, 2.85914 50.92541, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1500353</td>\n",
       "      <td>1500353-4</td>\n",
       "      <td>7375.198208</td>\n",
       "      <td>2022-10-15 10:38:17</td>\n",
       "      <td>POLYGON ((5.59302 50.79991, 5.59303 50.80009, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1462095</td>\n",
       "      <td>1462095-9</td>\n",
       "      <td>1630.024640</td>\n",
       "      <td>2022-09-21 17:15:03</td>\n",
       "      <td>POLYGON ((5.00341 50.83186, 5.00342 50.83204, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1571140</td>\n",
       "      <td>1571140-4</td>\n",
       "      <td>9685.894003</td>\n",
       "      <td>2022-10-03 18:42:00</td>\n",
       "      <td>POLYGON ((5.16576 51.30617, 5.16576 51.30635, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1369340</td>\n",
       "      <td>1369340-2</td>\n",
       "      <td>8770.054389</td>\n",
       "      <td>2022-10-18 06:50:47</td>\n",
       "      <td>POLYGON ((3.37756 51.02309, 3.37756 51.02327, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1671783</td>\n",
       "      <td>1671783-2</td>\n",
       "      <td>8306.438809</td>\n",
       "      <td>2022-09-20 18:30:19</td>\n",
       "      <td>POLYGON ((5.07273 50.81302, 5.07274 50.8132, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1454312</td>\n",
       "      <td>1454312-1</td>\n",
       "      <td>8911.643894</td>\n",
       "      <td>2022-10-18 08:41:00</td>\n",
       "      <td>POLYGON ((5.13133 51.26921, 5.13134 51.26939, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   parentname   fieldname  median_yield                date  \\\n",
       "0     1574374   1574374-3   8259.734336 2022-09-21 18:54:15   \n",
       "1     1422963  1422963-10   1030.794709 2022-10-25 10:53:03   \n",
       "2     1676341   1676341-3   2266.873831 2022-10-12 14:44:10   \n",
       "3     1650385   1650385-7   1438.154486 2022-09-30 14:21:27   \n",
       "4     1500353   1500353-4   7375.198208 2022-10-15 10:38:17   \n",
       "..        ...         ...           ...                 ...   \n",
       "95    1462095   1462095-9   1630.024640 2022-09-21 17:15:03   \n",
       "96    1571140   1571140-4   9685.894003 2022-10-03 18:42:00   \n",
       "97    1369340   1369340-2   8770.054389 2022-10-18 06:50:47   \n",
       "98    1671783   1671783-2   8306.438809 2022-09-20 18:30:19   \n",
       "99    1454312   1454312-1   8911.643894 2022-10-18 08:41:00   \n",
       "\n",
       "                                             geometry  \n",
       "0   POLYGON ((3.18533 51.01671, 3.18533 51.01689, ...  \n",
       "1   POLYGON ((5.11628 51.31606, 5.11629 51.31624, ...  \n",
       "2   POLYGON ((5.03693 51.28708, 5.03693 51.28725, ...  \n",
       "3   POLYGON ((2.85914 50.92523, 2.85914 50.92541, ...  \n",
       "4   POLYGON ((5.59302 50.79991, 5.59303 50.80009, ...  \n",
       "..                                                ...  \n",
       "95  POLYGON ((5.00341 50.83186, 5.00342 50.83204, ...  \n",
       "96  POLYGON ((5.16576 51.30617, 5.16576 51.30635, ...  \n",
       "97  POLYGON ((3.37756 51.02309, 3.37756 51.02327, ...  \n",
       "98  POLYGON ((5.07273 50.81302, 5.07274 50.8132, 5...  \n",
       "99  POLYGON ((5.13133 51.26921, 5.13134 51.26939, ...  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpd.read_file(input_df1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 13:47:17,129|extraction_pipeline|INFO:  Loading input dataframe from /mnt/CEPH_PROJECTS/ScaleAgData/06_abhishek/few_shot/scaleag-vito-v1/resources/Fieldwork_2023_samples_uid.geojson.\n",
      "2025-12-17 13:47:17,141|extraction_pipeline|INFO:  Preparing the job dataframe.\n",
      "2025-12-17 13:47:17,142|extraction_pipeline|INFO:  Performing splitting by s2 grid...\n",
      "/home/absingh/miniconda3/lib/python3.9/site-packages/openeo_gfmap/manager/job_splitters.py:113: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  polygons[\"centroid\"] = polygons.geometry.centroid\n",
      "2025-12-17 13:47:17,442|extraction_pipeline|INFO:  Dataframes split to jobs, creating the job dataframe...\n",
      "  0%|                                                                                                                        | 0/6 [00:00<?, ?it/s]/mnt/CEPH_PROJECTS/ScaleAgData/06_abhishek/few_shot/scaleag-vito-v1/notebooks/scaleagdata_vito/openeo/extract_sample_scaleag.py:149: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  job[\"lat\"] = job.geometry.centroid.y\n",
      "/mnt/CEPH_PROJECTS/ScaleAgData/06_abhishek/few_shot/scaleag-vito-v1/notebooks/scaleagdata_vito/openeo/extract_sample_scaleag.py:150: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  job[\"lon\"] = job.geometry.centroid.x\n",
      "/mnt/CEPH_PROJECTS/ScaleAgData/06_abhishek/few_shot/scaleag-vito-v1/notebooks/scaleagdata_vito/openeo/extract_sample_scaleag.py:149: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  job[\"lat\"] = job.geometry.centroid.y\n",
      "/mnt/CEPH_PROJECTS/ScaleAgData/06_abhishek/few_shot/scaleag-vito-v1/notebooks/scaleagdata_vito/openeo/extract_sample_scaleag.py:150: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  job[\"lon\"] = job.geometry.centroid.x\n",
      "/mnt/CEPH_PROJECTS/ScaleAgData/06_abhishek/few_shot/scaleag-vito-v1/notebooks/scaleagdata_vito/openeo/extract_sample_scaleag.py:149: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  job[\"lat\"] = job.geometry.centroid.y\n",
      "/mnt/CEPH_PROJECTS/ScaleAgData/06_abhishek/few_shot/scaleag-vito-v1/notebooks/scaleagdata_vito/openeo/extract_sample_scaleag.py:150: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  job[\"lon\"] = job.geometry.centroid.x\n",
      "/mnt/CEPH_PROJECTS/ScaleAgData/06_abhishek/few_shot/scaleag-vito-v1/notebooks/scaleagdata_vito/openeo/extract_sample_scaleag.py:149: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  job[\"lat\"] = job.geometry.centroid.y\n",
      "/mnt/CEPH_PROJECTS/ScaleAgData/06_abhishek/few_shot/scaleag-vito-v1/notebooks/scaleagdata_vito/openeo/extract_sample_scaleag.py:150: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  job[\"lon\"] = job.geometry.centroid.x\n",
      "/mnt/CEPH_PROJECTS/ScaleAgData/06_abhishek/few_shot/scaleag-vito-v1/notebooks/scaleagdata_vito/openeo/extract_sample_scaleag.py:149: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  job[\"lat\"] = job.geometry.centroid.y\n",
      "/mnt/CEPH_PROJECTS/ScaleAgData/06_abhishek/few_shot/scaleag-vito-v1/notebooks/scaleagdata_vito/openeo/extract_sample_scaleag.py:150: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  job[\"lon\"] = job.geometry.centroid.x\n",
      " 83%|█████████████████████████████████████████████████████████████████████████████████████████████▎                  | 5/6 [00:00<00:00, 43.72it/s]/mnt/CEPH_PROJECTS/ScaleAgData/06_abhishek/few_shot/scaleag-vito-v1/notebooks/scaleagdata_vito/openeo/extract_sample_scaleag.py:149: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  job[\"lat\"] = job.geometry.centroid.y\n",
      "/mnt/CEPH_PROJECTS/ScaleAgData/06_abhishek/few_shot/scaleag-vito-v1/notebooks/scaleagdata_vito/openeo/extract_sample_scaleag.py:150: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  job[\"lon\"] = job.geometry.centroid.x\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 43.43it/s]\n",
      "2025-12-17 13:47:17,585|extraction_pipeline|INFO:  Job dataframe created with 6 jobs.\n",
      "2025-12-17 13:47:17,586|extraction_pipeline|INFO:  Setting up the extraction functions.\n",
      "2025-12-17 13:47:17,586|extraction_pipeline|INFO:  Initializing the job manager.\n",
      "2025-12-17 13:47:17,587|extraction_pipeline|INFO:  Launching the jobs manager.\n",
      "2025-12-17 13:47:17,587|openeo_gfmap.manager|INFO:  Starting ThreadPoolExecutor with 4 workers.\n",
      "2025-12-17 13:47:17,587|openeo_gfmap.manager|INFO:  Creating and running jobs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated using refresh token.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 13:56:59,541|openeo_gfmap.manager|INFO:  Parsed item timeseries.parquet from job j-2512171247204ea399b165eea6c8aaa3\n",
      "2025-12-17 13:56:59,591|openeo_gfmap.manager|INFO:  Adding 1 items to the STAC collection...\n",
      "2025-12-17 13:56:59,591|openeo_gfmap.manager|INFO:  Job j-2512171247204ea399b165eea6c8aaa3 and post job action finished successfully.\n",
      "2025-12-17 13:58:20,791|openeo_gfmap.manager|INFO:  Parsed item timeseries.parquet from job j-2512171247374abbaf365800f3822b1f\n",
      "2025-12-17 13:58:20,839|openeo_gfmap.manager|INFO:  Adding 1 items to the STAC collection...\n",
      "2025-12-17 13:58:20,839|openeo_gfmap.manager|INFO:  Job j-2512171247374abbaf365800f3822b1f and post job action finished successfully.\n",
      "2025-12-17 14:06:47,961|openeo_gfmap.manager|INFO:  Parsed item timeseries.parquet from job j-251217125701462cb0b5320be2a231f4\n",
      "2025-12-17 14:06:48,011|openeo_gfmap.manager|INFO:  Adding 1 items to the STAC collection...\n",
      "2025-12-17 14:06:48,012|openeo_gfmap.manager|INFO:  Job j-251217125701462cb0b5320be2a231f4 and post job action finished successfully.\n",
      "2025-12-17 14:09:10,444|openeo_gfmap.manager|INFO:  Parsed item timeseries.parquet from job j-25121712582246fb83ff6cc512c2ac5d\n",
      "2025-12-17 14:09:10,490|openeo_gfmap.manager|INFO:  Adding 1 items to the STAC collection...\n",
      "2025-12-17 14:09:10,491|openeo_gfmap.manager|INFO:  Job j-25121712582246fb83ff6cc512c2ac5d and post job action finished successfully.\n",
      "2025-12-17 14:15:36,148|openeo_gfmap.manager|INFO:  Parsed item timeseries.parquet from job j-25121713064941649473c10c41033535\n",
      "2025-12-17 14:15:36,196|openeo_gfmap.manager|INFO:  Adding 1 items to the STAC collection...\n",
      "2025-12-17 14:15:36,197|openeo_gfmap.manager|INFO:  Job j-25121713064941649473c10c41033535 and post job action finished successfully.\n",
      "2025-12-17 14:25:42,931|openeo_gfmap.manager|INFO:  Parsed item timeseries.parquet from job j-2512171309124ba28480db870c380929\n",
      "2025-12-17 14:25:42,976|openeo_gfmap.manager|INFO:  Adding 1 items to the STAC collection...\n",
      "2025-12-17 14:25:42,977|openeo_gfmap.manager|INFO:  Job j-2512171309124ba28480db870c380929 and post job action finished successfully.\n",
      "2025-12-17 14:26:43,001|openeo_gfmap.manager|INFO:  Quitting job tracking & waiting for last post-job actions to finish.\n",
      "2025-12-17 14:26:43,002|openeo_gfmap.manager|INFO:  Exiting ThreadPoolExecutor.\n",
      "2025-12-17 14:26:43,002|openeo_gfmap.manager|INFO:  All jobs finished running.\n",
      "2025-12-17 14:26:43,002|openeo_gfmap.manager|INFO:  STAC was disabled, skipping generation of the catalogue.\n",
      "2025-12-17 14:26:43,003|extraction_pipeline|INFO:  Extraction completed successfully.\n"
     ]
    }
   ],
   "source": [
    "job_params = dict(\n",
    "    output_folder=output_folder,\n",
    "    input_df=input_df,\n",
    "    unique_id_column=unique_id_column,\n",
    "    composite_window=composite_window,\n",
    ")\n",
    "extract(generate_input_for_extractions(job_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset will be extracted, it can be loaded with the `load_dataset` function by specifying the path where the `.parquet` files have been downloaded. Moreover, the following manipulations of the dataset are also possible:\n",
    "\n",
    "- `window_of_interest`: the user can specify a time window of interest out of the whole available time-series. `start_date` and `end_date` should be provided as strings in a list.\n",
    "- `use_valid_time`: the user might want to define the window of interest based on the `date` the label is associated with. If so, also `required_min_timesteps` should be provided\n",
    "- `buffer_window`: buffers the `start_date` and `end_date` by the number of time steps here specified  \n",
    "\n",
    "In the following cell, we load the extracted dataset for 1 year of data.\n",
    "\n",
    "**NOTE:** this code currently assumes that we are dealing with 1 year of data falling in the same time period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Presto datasets initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 23.49it/s]\n"
     ]
    }
   ],
   "source": [
    "df = load_dataset(\n",
    "    files_root_dir=output_folder,\n",
    "    window_of_interest=[start_date, end_date],\n",
    "    composite_window=composite_window,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following step splits the data into train, test and val datasets. the split can be performed by uniform sampling or by group sampling. The former is usually more suitable for the binary and multiclass classification tasks, to ensure the data distribution is represented in all the 3 sets. The latter is more specific for cases where we want to avoid data autocorrelation and so data leakage between training and val/test sets.\n",
    "In the case of yield estimation, for instance, we often have samples coming from the same field. So we might want to separate the data based on the field they belong to to better test the model generalization capabilities.\n",
    "Therefore:\n",
    "- `uniform_sample_by`: pass the name of the column in the dataframe to perform uniform sampling on\n",
    "- `group_sample_by`: pass the name of the column in the dataframe to perform the group sampling on\n",
    "\n",
    "`sampling_fraction` indicates the proportion of the training set out of the whole dataset. the remaining percentage will be equally devided into validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-17 14:33:14.364\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mscaleagdata_vito.presto.utils\u001b[0m:\u001b[36mtrain_test_val_split\u001b[0m:\u001b[36m565\u001b[0m - \u001b[1mTraining set size: 227\u001b[0m\n",
      "\u001b[32m2025-12-17 14:33:14.365\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mscaleagdata_vito.presto.utils\u001b[0m:\u001b[36mtrain_test_val_split\u001b[0m:\u001b[36m566\u001b[0m - \u001b[1mValidation set size: 28\u001b[0m\n",
      "\u001b[32m2025-12-17 14:33:14.365\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mscaleagdata_vito.presto.utils\u001b[0m:\u001b[36mtrain_test_val_split\u001b[0m:\u001b[36m567\u001b[0m - \u001b[1mTest set size: 29\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#df_train, df_val, df_test = train_test_val_split(df=df, group_sample_by=\"parentname\", sampling_frac=0.8)\n",
    "df_train, df_val, df_test = train_test_val_split(df=df, group_sample_by=\"uid\", sampling_frac=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set up the parameters needed for initializing presto datasets for the specific task:\n",
    "- `num_timesteps`: can be inferred by the max number of the `available_timesteps` \n",
    "- `target_name`: name of the column containing the target data\n",
    "- `upper_bound` and `lower_bound`: these should be set to the min and max of the distribution. Therefore, it is important to get rid of potential outlaiers beforehand.\n",
    "\n",
    "**NOTE:** upper and lower bounds are also used to normalize the targets during the training process. Therefore it is important to keep track of such values to convert the predictions to the original units in the inference step!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize distribution to check for outliers to exclude if needed\n",
    "num_timesteps = df.available_timesteps.max()\n",
    "task_type = \"regression\"\n",
    "target_name = \"AGB_t/ha\"\n",
    "upper_bound = df[target_name].max() \n",
    "lower_bound = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Initialize the training, validation and test datasets objects to be used for training Presto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-17 14:34:07.042\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mscaleagdata_vito.presto.datasets_prometheo\u001b[0m:\u001b[36mset_num_outputs\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mSetting number of outputs to 1 for regression task.\u001b[0m\n",
      "\u001b[32m2025-12-17 14:34:07.044\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mscaleagdata_vito.presto.datasets_prometheo\u001b[0m:\u001b[36mset_num_outputs\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mSetting number of outputs to 1 for regression task.\u001b[0m\n",
      "\u001b[32m2025-12-17 14:34:07.046\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mscaleagdata_vito.presto.datasets_prometheo\u001b[0m:\u001b[36mset_num_outputs\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mSetting number of outputs to 1 for regression task.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# initialize datasets\n",
    "train_ds = ScaleAgDataset(\n",
    "    df_train,\n",
    "    num_timesteps=num_timesteps,\n",
    "    task_type=task_type,\n",
    "    target_name=target_name,\n",
    "    composite_window=composite_window,\n",
    "    upper_bound=upper_bound,\n",
    "    lower_bound=lower_bound,\n",
    ")\n",
    "val_ds = ScaleAgDataset(\n",
    "    df_val,\n",
    "    num_timesteps=num_timesteps,\n",
    "    task_type=task_type,\n",
    "    target_name=target_name,\n",
    "    composite_window=composite_window,\n",
    "    upper_bound=upper_bound,\n",
    "    lower_bound=lower_bound,\n",
    ")\n",
    "test_ds = ScaleAgDataset(\n",
    "    df_test,\n",
    "    num_timesteps=num_timesteps,\n",
    "    task_type=task_type,\n",
    "    target_name=target_name,\n",
    "    composite_window=composite_window,\n",
    "    upper_bound=upper_bound,\n",
    "    lower_bound=lower_bound,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Presto Finetuning\n",
    "\n",
    "In this section Presto will be Fine-Tuned in a supervised way for the target downstream task. first we set up the following experiment parameters:\n",
    "\n",
    "- `output_dir` : where to dave the model \n",
    "- `experiment_name` : the model name\n",
    "- `pretrained_model_path` : pretrained presto model to start the fine tuning from. Can be a string indicating the path to the model or a url "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set models hyperparameters\n",
    "model_output_dir = Path(\"/mnt/CEPH_PROJECTS/ScaleAgData/06_abhishek/few_shot/output_AGB/\")\n",
    "experiment_name = \"presto-agb-dekv1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-17 14:55:23.964\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mscaleagdata_vito.presto.utils\u001b[0m:\u001b[36mfinetune_on_task\u001b[0m:\u001b[36m460\u001b[0m - \u001b[1mFinetuning the model on regression task\u001b[0m\n",
      "\u001b[32m2025-12-17 14:55:23.966\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprometheo.finetune\u001b[0m:\u001b[36m_setup\u001b[0m:\u001b[36m223\u001b[0m - \u001b[1mUsing output dir: /mnt/CEPH_PROJECTS/ScaleAgData/06_abhishek/few_shot/output_AGB\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71dc2287286a4a8699dd31dce1384b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finetuning:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b99bd4c0d5c44b48815b349f7458b019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-17 14:55:24.385\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprometheo.finetune\u001b[0m:\u001b[36m_train_loop\u001b[0m:\u001b[36m178\u001b[0m - \u001b[1mPROGRESS after Epoch 1/50: Epoch 1/50 | Train Loss: 0.0022 | Val Loss: 0.1403 | Best Loss: 0.1403 (improved)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be54acad6ad4f389685815fcc04acc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-17 14:55:24.773\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprometheo.finetune\u001b[0m:\u001b[36m_train_loop\u001b[0m:\u001b[36m178\u001b[0m - \u001b[1mPROGRESS after Epoch 2/50: Epoch 2/50 | Train Loss: 0.0016 | Val Loss: 0.0931 | Best Loss: 0.0931 (improved)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be464482962847b88e5deb0284e26c15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-17 14:55:25.166\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprometheo.finetune\u001b[0m:\u001b[36m_train_loop\u001b[0m:\u001b[36m178\u001b[0m - \u001b[1mPROGRESS after Epoch 3/50: Epoch 3/50 | Train Loss: 0.0012 | Val Loss: 0.0609 | Best Loss: 0.0609 (improved)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd6491b9c5764f5da3bb50497af6f441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-17 14:55:25.575\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprometheo.finetune\u001b[0m:\u001b[36m_train_loop\u001b[0m:\u001b[36m178\u001b[0m - \u001b[1mPROGRESS after Epoch 4/50: Epoch 4/50 | Train Loss: 0.0007 | Val Loss: 0.0418 | Best Loss: 0.0418 (improved)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9045952e3e441c985ebe50e57e7557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-17 14:55:25.967\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprometheo.finetune\u001b[0m:\u001b[36m_train_loop\u001b[0m:\u001b[36m178\u001b[0m - \u001b[1mPROGRESS after Epoch 5/50: Epoch 5/50 | Train Loss: 0.0005 | Val Loss: 0.0332 | Best Loss: 0.0332 (improved)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9535581fac4a437fa94f8147654f2c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-17 14:55:26.352\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprometheo.finetune\u001b[0m:\u001b[36m_train_loop\u001b[0m:\u001b[36m178\u001b[0m - \u001b[1mPROGRESS after Epoch 6/50: Epoch 6/50 | Train Loss: 0.0004 | Val Loss: 0.0317 | Best Loss: 0.0317 (improved)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b176aecdc294412d8be25365f6671bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-17 14:55:26.754\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprometheo.finetune\u001b[0m:\u001b[36m_train_loop\u001b[0m:\u001b[36m178\u001b[0m - \u001b[1mPROGRESS after Epoch 7/50: Epoch 7/50 | Train Loss: 0.0004 | Val Loss: 0.0335 | Best Loss: 0.0317 (no improvement for 1 epochs)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "894fe88c2e1a4284ac6f8c4f4b33fa3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-17 14:55:27.143\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprometheo.finetune\u001b[0m:\u001b[36m_train_loop\u001b[0m:\u001b[36m178\u001b[0m - \u001b[1mPROGRESS after Epoch 8/50: Epoch 8/50 | Train Loss: 0.0004 | Val Loss: 0.0358 | Best Loss: 0.0317 (no improvement for 2 epochs)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6111dc839a294a65a2a94623cabc8971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-17 14:55:27.524\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprometheo.finetune\u001b[0m:\u001b[36m_train_loop\u001b[0m:\u001b[36m161\u001b[0m - \u001b[1mEarly stopping!\u001b[0m\n",
      "\u001b[32m2025-12-17 14:55:27.543\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprometheo.finetune\u001b[0m:\u001b[36mrun_finetuning\u001b[0m:\u001b[36m327\u001b[0m - \u001b[1mFinetuning done\u001b[0m\n",
      "\u001b[32m2025-12-17 14:55:27.544\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mscaleagdata_vito.presto.utils\u001b[0m:\u001b[36mevaluate_finetuned_model\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mEvaluating the finetuned model on regression task\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'RMSE': 2.546637120283533,\n",
       " 'R2_score': -0.2759532928466797,\n",
       " 'explained_var_score': -0.27283310890197754,\n",
       " 'MAPE': 5.678121566772461}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct the model with finetuning head starting from the pretrained model\n",
    "finetuned_model = finetune_on_task(\n",
    "    train_ds=train_ds,\n",
    "    val_ds=val_ds,\n",
    "    pretrained_model_path=get_pretrained_model_url(composite_window=composite_window),\n",
    "    output_dir=model_output_dir, \n",
    "    experiment_name=experiment_name,\n",
    "    num_workers=0,\n",
    "    )\n",
    "evaluate_finetuned_model(finetuned_model, test_ds, num_workers=0, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inference using Fine-Tuned end-to-end Presto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we apply the fine tuned model to generate a yield map on an unseen area. \n",
    "We need to indicate the spatial and temporal extent. The 2 cells below, offer a simple way for the user to provide these information and perform once again the extraction from CDSE of the EO time-series required by Presto. \n",
    "We also need to indicate the `output_dir` of where to save the datacube of the extraction, its `output_filename` and the `composite_window` which will be the same as used for finetuning the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = ui_map(area_limit=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select 1 year of data\n",
    "slider = date_slider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"/home/giorgia/Private/data/scaleag/demo/regression\")\n",
    "output_filename = \"inference_area\"\n",
    "inference_file = output_dir / f\"{output_filename}.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_inputs_for_inference(\n",
    "    spatial_extent=map.get_extent(),\n",
    "    temporal_extent=slider.get_processing_period(),\n",
    "    output_path=output_dir,\n",
    "    output_filename=f\"{output_filename}.nc\",\n",
    "    composite_window=composite_window,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the datacube has been extracted, we can perform the inference task using the finetuned model and visualize the predicted map. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_file = get_resources_dir() / \"inference_area_tevuren.nc\"\n",
    "mask_path = get_resources_dir() / \"LPIS_flanders_potatoes_2022.tif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = load_finetuned_model(model_output_dir / experiment_name, task_type=task_type)\n",
    "presto_model = PrestoPredictor(\n",
    "    model=finetuned_model,\n",
    "    batch_size=50,\n",
    "    task_type=task_type,\n",
    "    composite_window=composite_window,\n",
    ")\n",
    "\n",
    "predictions = presto_model.predict(inference_file, upper_bound=upper_bound, lower_bound=lower_bound, mask_path=mask_path)\n",
    "predictions_map = reshape_result(predictions, path_to_input_file=inference_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(prob_map=predictions_map, path_to_input_file=inference_file, task=task_type, ts_index=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary task: crop/no-crop\n",
    "\n",
    "Now we test the few-shot learning on a binary task. We Fine-Tune presto on datapoints sampled from Flanders on 2021. This time, the dataset is the result of a monthly compositing. We initialize the parameters for the dataset preparation accordingly  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = Path(\"/home/giorgia/Private/data/scaleag/demo/worldcereal/\")\n",
    "input_df = get_resources_dir() / \"dummy_cropland.geojson\"\n",
    "task_type = \"binary\"\n",
    "target_name = \"LANDCOVER_LABEL\"\n",
    "composite_window = \"month\"\n",
    "unique_id_column = \"sample_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_params = dict(\n",
    "    output_folder=output_folder,\n",
    "    input_df=input_df,\n",
    "    unique_id_column=unique_id_column,\n",
    "    composite_window=composite_window,\n",
    ")\n",
    "extract(generate_input_for_extractions(job_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Presto datasets initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset(\n",
    "    files_root_dir=output_folder,\n",
    "    composite_window=composite_window,\n",
    ")\n",
    "train_df, val_df, test_df = train_test_val_split(df=df, uniform_sample_by=target_name, sampling_frac=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of a binary classification task, specifying the upper and lower bounds won't be necessary anymore. In case we are dealing with multiclass labels, we can convert the problem into a binary classification task by providing the `positive_labels` argument to the `ScaleAgDatset` class. The list of labels passed as value to `positive_labels` indicates which subset of classes should be interpreted as positive class (here \"crop\"). All the other labels will therefore be interpreted as negative class (here \"no-crop\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_timesteps = df.available_timesteps.max()\n",
    "positive_labels = [10, 11, 12, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ScaleAgDataset(\n",
    "    train_df,\n",
    "    num_timesteps=num_timesteps,\n",
    "    task_type=task_type,\n",
    "    target_name=target_name,\n",
    "    positive_labels=positive_labels,\n",
    "    composite_window=composite_window,\n",
    ")\n",
    "\n",
    "val_ds = ScaleAgDataset(\n",
    "    val_df,\n",
    "    num_timesteps=num_timesteps,\n",
    "    task_type=task_type,\n",
    "    target_name=target_name,\n",
    "    positive_labels=positive_labels,\n",
    "    composite_window=composite_window,\n",
    ")\n",
    "\n",
    "test_ds = ScaleAgDataset(\n",
    "    test_df,\n",
    "    num_timesteps=num_timesteps,\n",
    "    task_type=task_type,\n",
    "    target_name=target_name,\n",
    "    positive_labels=positive_labels,\n",
    "    composite_window=composite_window,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Presto Finetuning\n",
    "\n",
    "In this section Presto will be Fine-Tuned in a supervised way for the target downstream task. Once again, we set up the experiment parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"presto_wc_ft_crop\"\n",
    "model_output_dir = Path(\"/home/giorgia/Private/data/scaleag/demo/worldcereal/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = finetune_on_task(\n",
    "                train_ds=train_ds,\n",
    "                val_ds=val_ds,\n",
    "                pretrained_model_path=get_pretrained_model_url(composite_window=composite_window),\n",
    "                output_dir=model_output_dir, \n",
    "                experiment_name=experiment_name,\n",
    "                num_workers=0,\n",
    "            )\n",
    "evaluate_finetuned_model(finetuned_model, test_ds, num_workers=0, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inference using Fine-Tuned end-to-end Presto\n",
    "\n",
    "We now apply the finetuned model to an unseen area to perform the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = load_finetuned_model(model_output_dir / experiment_name, task_type=task_type)\n",
    "inference_file = get_resources_dir() / \"worldcereal_preprocessed_inputs.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presto_model = PrestoPredictor(\n",
    "    model=finetuned_model,\n",
    "    batch_size=50,\n",
    "    task_type=task_type,\n",
    "    composite_window=composite_window,\n",
    ")\n",
    "\n",
    "predictions = presto_model.predict(inference_file)\n",
    "prob_map = reshape_result(predictions, path_to_input_file=inference_file)\n",
    "pred_map = presto_model.get_predictions(prob_map, threshold=0.75)\n",
    "plot_results(prob_map=prob_map, pred_map=pred_map, path_to_input_file=inference_file, task=task_type, ts_index=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
